---
title: On New Jersey's Opioid Problem
author: ''
date: '2020-05-11'
slug: on-new-jerseys-opioid-problem
categories: []
tags:
  - R
  - multilevel modeling
  - plotly
  - ARCOS
  - opioids
summary: I was raised in New Jersey and (at this point, like many people) have friends and family impacted by opioid addiction. I've read news reports on the prescription opioid epidemic, but I've never done a deep dive into the data behind the epidemic to learn more.
---


```{r include=FALSE}
library(here)
library(plotly)
library(dplyr)
library(stringr)
library(widgetframe)
library(DT)
library(tidycensus)
library(purrr)
library(tidyr)
library(ggplot2)
library(htmltools)

knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)

display_arcos_plot <- function(rds_file, static = FALSE) {
  plt <- 
    readRDS(here("static", "post", "arcos-project-post", "plot-objects", rds_file))
  
  if (!static) {
    frameWidget(plt, height = 450)  
  } else {
    plt
  }
}

read_arcos_data <- function(rds_file) {
  readRDS(here("static", "post", "arcos-project-post", "data", rds_file))  
}

load_total_nj_pop <- function() {
  fname <-
    here("static", "post", "arcos-project-post", "data", "total-nj-pop.csv")
  
  if (file.exists(fname)) {
    read.csv(fname, stringsAsFactors = FALSE)
  } else {
   pop_df <-
     map_df(2005:2016, function(yr) {
       get_acs(geography = "state",
               variables = "B01003_001", 
               year = yr,
               state = "NJ",
               survey = "acs1") %>% 
         mutate(year = yr) %>% 
         rename(total_population = estimate)
     }) %>% 
     select(Geography = NAME, year, total_population)
   
   write.csv(pop_df, fname, row.names = FALSE)
   pop_df
  }
}

nj_pop <- load_total_nj_pop()

nj_data <-
  read.csv(here("static", "post", "arcos-project-post", "data", "nj-regression-data.csv"),
           stringsAsFactors = FALSE)
```


I was raised in New Jersey and (at this point, like many people) have friends and family impacted by opioid addiction. I've read news reports on the prescription opioid epidemic, but I've never done a deep dive into the data behind the epidemic to learn more. 

After some research, I found the <a href="https://wpinvestigative.github.io/arcos/" target="_blank">Automation of Reports and Consolidated Orders System (ARCOS) database</a>. The data from ARCOS is owned by the Drug Enforcement Agency (DEA), but was released by the Washington Post on July 16th, 2019, after a long legal battle in conjunction with the Charleston Gazette-Mail of West Virginia. ARCOS tracks the supply side of opioid pills in the United States, with data on every prescription opioid pill sold from a manufacturer to a pharmacy from 2006 to 2014.

Focusing on my home state, I wanted to understand how much the supply of opioid pills is associated with negative public health outcomes. I discovered that there is indeed an association - <strong>in particular, an increase in the number of pills supplied per capita to a county in New Jersey is associated with more prescription opioid related hospitalizations.</strong>  The rest of this post is focused on how I came to that conclusion and the nuance surrounding it. 

The opioid epidemic is far reaching, and the ARCOS database is an invaluable resource for understanding it. For national coverage and analysis of ARCOS, check out the <a href="https://www.washingtonpost.com/national/2019/07/20/opioid-files/?arc404=true" target="_blank">Washington Post's collection of articles</a>. For local reports on ARCOS, check out the Washington Post's <a href="https://www.washingtonpost.com/national/2019/08/12/post-released-deas-data-pain-pills-heres-what-local-journalists-are-using-it/" target="_blank">other collection of articles</a>. For an existing analysis of this data for New Jersey, I highly recommend <a href="https://www.nj.com/data/2019/07/tracking-the-15-billion-painkillers-that-flooded-nj-and-sparked-an-epidemic-thats-killed-nearly-20k.html" target="_blank">NJ.com's article</a>, which is filled with some excellent data visualizations and reporting that offers a great summary of how badly impacted New Jersey has been by the flood of prescription pills into its communities. 

All code and data for the analyses from this post can be found at the associated <a href="https://github.com/svitkin/arcos-project" target="_blank">github repository</a>.

## About the Data
### Hospitlizations
I model the impact of the opioid pill supply on New Jersey by trying to find its effect on a negative public health outcome, prescription opioid related hospital visits. My testable hypothesis is: <strong>An increase in the number of opioid pills supplied per capita to New Jersey counties is associated with an increased number of prescription opioid related hospital visits.</strong>

NJ.gov has a <a href="https://www.nj.gov/health/populationhealth/opioid/opioid_hospital.shtml" target="_blank">data dashboard on drug-related hospital visits</a> from 2008 to 2018[^1], which has enough overlap with the years of ARCOS data to be usable. It is also one of the few public health outcomes I could find with full data, as many counts (like opioid related mortality) are suppressed for privacy reasons. 

<div style="margin-bottom:50px;"></div>

```{r}
total_nj_summary_hosp <-
  nj_data %>% 
  mutate(buyer_county = "All of New Jersey") %>% 
  filter(year %in% 2008:2015) %>% 
  inner_join(nj_pop) %>% 
  group_by(year) %>% 
  mutate(total_hosp_year = sum(opioid_visit_count),
         hosp_per_10 = total_hosp_year/(total_population/10000)) %>% 
  ungroup() %>% 
  distinct(buyer_county, year, total_hosp_year, hosp_per_10) %>% 
  group_by(buyer_county) %>% 
  summarise(total_hosp = sum(total_hosp_year),
            mean_hosp_per_10 = round(mean(hosp_per_10), 2)) %>% 
  ungroup()
  
county_nj_summary_hosp <-
  nj_data %>% 
  filter(year %in% 2008:2015) %>% 
  group_by(buyer_county) %>% 
  summarise(total_hosp = sum(opioid_visit_count),
            mean_hosp_per_10 = round(mean(total_hosp/(population/10000)), 2)) %>% 
  ungroup()

bind_rows(total_nj_summary_hosp,
          county_nj_summary_hosp) %>% 
  mutate(buyer_county = as.factor(buyer_county)) %>% 
  datatable(filter = "top",
            colnames = 
              c("County", "Total # of Prescription Opioid Related Hospitalizations", "Yearly Average Prescription Opioid Related Hospitalizations per 10,000 Residents"),
            caption = htmltools::tags$caption(
              style = 'caption-side: bottom; text-align: center;',
              htmltools::withTags(
              htmltools::HTML("<em>Prescription Opioid Related Hospitalizations in New Jersey (2008 - 2015)</em><br/><sup>Data Source: <a href='https://www.nj.gov/health/populationhealth/opioid/opioid_hospital.shtml' target='_blank'> Drug-related Hospital Visits Dashboard</a> and <a href='https://cran.r-project.org/web/packages/tidycensus/index.html' target='_blank'>tidycensus R package</a></sup>"))),
            rownames = FALSE,
            extensions = 'Buttons', 
            options = list(
              dom = 'Bfrtip',
              buttons = c('copy', 'csv', 'pdf'),
              paging = FALSE,
              scrollY = "200px"
            )) %>% 
  formatCurrency(2, currency = "", digits = 0) %>% 
  frameWidget(height = 450)
```


Every county in New Jersey has a positive correlation between year and number of hospitalizations, indicating a general increase in prescription opioid related hospitalizations over time.

```{r}
display_arcos_plot("x_of_hospital_visits_per_10_000_residents-county.rds")
```

### ARCOS

The ARCOS API wrapper in R has a handy <code>arcos::summarized_county_annual()</code> function that pulls in the number of hydrocodone and oxycodone pills supplied to a county per year. Going forward, when I refer to opioid pills in the context of the ARCOS dataset, I only mean oxycodone and hydrocodone pills. 

The  years of hospitalization data span from 2008 to 2015, while the ARCOS data goes from 2006 to 2014. This presented an issue when combining data for the model. If I simply took the number of county-year observations from the overlap of these two datasets, there would only be 147 observations for the 21 counties in New Jersey, a lot less data than the full 9 years of ARCOS data. So partially motivated by trying to maximize the number of observations, and partially because there could legitimately be a lag in the effect of opiate pill supply (dependence on a substance that could lead to an eventual hospitalization takes some nonzero amount of time to develop), I lagged the ARCOS data by one year to match up with the hospitalization data. Despite that change, I still lose the 2006 year of ARCOS data, bringing us to 168 observations, 21 counties and 8 years of data, from 2008 to 2015.

<div style="margin-bottom:30px;"></div>

```{r}
total_nj_summary_supply <-
  nj_data %>% 
  mutate(buyer_county = "All of New Jersey") %>% 
  filter(year %in% 2006:2014) %>% 
  inner_join(nj_pop) %>% 
  group_by(year) %>% 
  mutate(total_pills_yr = sum(dosage_unit),
            pills_per_capita = total_pills_yr/total_population) %>% 
  ungroup() %>% 
  distinct(buyer_county, year, total_pills_yr, pills_per_capita) %>% 
  group_by(buyer_county) %>% 
  summarise(total_pills = sum(total_pills_yr),
            mean_pills = round(mean(pills_per_capita), 2)) %>% 
  ungroup()
  
county_nj_summary_supply <-
  nj_data %>% 
  filter(year %in% 2006:2014) %>% 
  group_by(buyer_county) %>% 
  summarise(total_pills = sum(dosage_unit),
            mean_pills = round(mean(dosage_unit/population), 2)) %>% 
  ungroup()

bind_rows(total_nj_summary_supply,
          county_nj_summary_supply) %>% 
  mutate(buyer_county = as.factor(buyer_county)) %>% 
  datatable(filter = "top",
            colnames = 
              c("County", "Total # of Pills", "Yearly Average # of Pills per Capita"),
            caption = htmltools::tags$caption(
              style = 'caption-side: bottom; text-align: center;',
              htmltools::withTags(
              htmltools::HTML("<em>Hydrocodone and Oxycodone Pills Supplied to New Jersey (2006 - 2014)</em><br/><sup>Data Source: <a href='https://CRAN.R-project.org/package=arcos' target='_blank'>ARCOS API</a> and <a href='https://cran.r-project.org/web/packages/tidycensus/index.html' target='_blank'>tidycensus R package</a></sup>"))),
            rownames = FALSE,
            extensions = 'Buttons', 
            options = list(
              dom = 'Bfrtip',
              buttons = c('copy', 'csv', 'pdf'),
              paging = FALSE,
              scrollY = "200px"
            )) %>% 
  formatCurrency(2, currency = "", digits = 0) %>% 
  frameWidget(height = 400)

```


From 2006 to 2014, New Jersey was supplied with `r formatC(total_nj_summary_supply$total_pills, format = "fg", big.mark = ",")` hydrocodone and oxycodone pills. On average, `r total_nj_summary_supply$mean_pills` hydrocodone and oxycodone pills were supplied per capita per year (per capita rates are obtained with county population estimates from the American Community Survey, discussed in greater detail below). There is quite a bit of range in the supply profile of counties, from `r county_nj_summary_supply %>% filter(mean_pills == min(mean_pills)) %>% pull(buyer_county)` County with `r min(county_nj_summary_supply$mean_pills)` pills per capita supplied on average to `r county_nj_summary_supply %>% filter(mean_pills == max(mean_pills)) %>% pull(buyer_county)` County with `r max(county_nj_summary_supply$mean_pills)` pills per capita supplied on average. However, common across all counties, is an upward trend in the number of pills per capita supplied over time. 

<div style="margin-bottom:20px;"></div>

```{r}
display_arcos_plot("opioid_pills_supplied_per_capita-county.rds")
```


Indeed, every single county in New Jersey has a positive correlation between opioid pills supplied per capita and year. A visual inspection of the relationship between pills supplied and hospital visits shows a generally positive association between the two quantities.

<div style="margin-bottom:20px;"></div>

```{r}
display_arcos_plot("opioid_pills_supplied_per_capita_in_previous_year-w-hosp_visits.rds")
```

### Prescriptions

I wanted my model of hopsitalizations to include the other side of the legal opioid supply, prescriptions. Data for opioid prescription rates by county (the number of prescriptions per 100 people) can be found on the <a href="https://www.cdc.gov/drugoverdose/maps/rxrate-maps.html" target="_blank">Center for Disease Control's (CDC) prescription rate maps</a>.[^2] For the eventual model of hospitalizations, prescription rate data is also lagged by one year to match up with the ARCOS data. 

<div style="margin-bottom:20px;"></div>

```{r}
display_arcos_plot("prescription_rate-county.rds")
```

Interestingly, although the supply to pharmacies of hydrocodone and oxycodone pills is increasing, the prescription rates do not always show the same obvious positive trend. 

<div style="margin-bottom:20px;"></div>

```{r}
display_arcos_plot("prescription_rate_in_previous_year-w-hosp_visits.rds")
```

Prescription rates, however, do seem to have a positive relationship with the number of prescription opioid-related hospital visits. The one group of visual outliers from this positive trend is Cape May. Out of all the counties, Cape May has the highest percentage of its population aged 60 years or older over the years of the data. It has an unusually high proportion of an age demographic that is more likely to get a prescriprion, but is less likely to be hospitalized for prescription opioid abuse (although the elderly can certainly abuse prescription opioids in a way that leads to hospitalization). An interesting point for future research.

### Controlling Variables

To model the effect of the number of opioid pills supplied accurately, I need to control for the other mechanisms that might impact the number of prescription opioid related hospitalizations. To best approximate these other factors, I pulled down demographic and economic characteristics of each county, so at least I can control for the changes in a county's "environment" (i.e. the make-up of its residents and its economic health).

For all demographic (and some economic) characteristics, I used the U.S. Census's American Community Survey (ACS) 1 year estimates, pulled down with the <a href="https://walkerke.github.io/tidycensus/" target="_blank">tidycensus</a> package. The ACS is continually sampled throughout its defined duration, with the 1 year ACS estimates having the smallest sample size of the available ACS datasets (the others being the 3 and 5 year). However, since we are interested in individual years and changes between those years, the 1 year estimates are more suited to our data (except for race categories in Cape May county in 2011, which had missing data for its 1 year estimate and was filled in with the 5 year estimate instead).[^3] I gathered distribution data on race, age, and median household income for each county from 2008 to 2015. Under the <a href="https://www.census.gov/content/dam/Census/library/publications/2018/acs/acs_general_handbook_2018_ch10.pdf" target="_blank">recommendation of the U.S. Census</a>, I used the annual averages from the <a href="https://www.bls.gov/cpi/research-series/home.htm" target="_blank">Consumer Price Index research series</a> to translate the median incomes to 2015 U.S. dollars.

Unemployment rates for each county, as another proxy for economic conditions, were pulled from the <a href="https://www.bls.gov/lau/" target="_blank">Bureau of Labor Statistics (BLS) Local Area Unemployment tables</a>.

Ideally, one would also want to control for how many residents have a legitimate medical need for opioid pills. This is a <a href="https://www.cdc.gov/drugoverdose/prescribing/guideline.html" target="_blank">complex question</a> with an inherent amount of uncertainty around how much pain is appropriate for someone to be prescribed opiates. Although it is not ideal, I use the Social Security Administration's data on the number of recipients of social security benefits for blindness and/or disability. This is not an ideal measurement, as this pools together numbers for many different disabilities and misses members of the population who are not receiving benefits but might have a medical need for an opioid prescription. However, it is currently the best approximation I could find for this concept.

## Random-Intercept Model

For reasons discussed in the Appendix, I chose to model prescription opioid hospitalizations with a random-intercept model.

The final set of data sources for this model are:

* ARCOS API
* NJ.gov Hospitalization Data Dashboard
* American Community Survey (ACS) 1-year estimates from the <code>tidycensus</code> package
* Bureau of Labor Statistics (BLS) Local Area Unemployment tables
* Center for Disease Control’s (CDC) prescription rate maps
* Social Security Administration's data on Blind and/or Disability Benefits

Each observation in the final dataset is a year within a county. I want the model to not only measure the yearly effect of a variable (i.e. the effect of prescription rate on hospiatlizations for a county in a given year), but also to estimate "between effects", differences between the average number of prescription opioid related hospitilizations for a county. To do this, I use the "within-between" formulation of the random-intercept model found in <a href="https://www.cambridge.org/core/services/aop-cambridge-core/content/view/0334A27557D15848549120FE8ECD8D63/S2049847014000077a.pdf/explaining_fixed_effects_random_effects_modeling_of_timeseries_crosssectional_and_panel_data.pdf" target="_blank">Bell and Jones, 2015</a>, itself a modification of the formula found in <a href="https://www.jstor.org/stable/1913646?seq=1" target="_blank">Mundlak, 1978</a>. I county-mean center all the variables, so that for example, each year of opioid supply data in Morris County has the mean number of opioid pills supplied per capita for Morris County subtracted from it. I also include the mean of almost every variable discussed to capture "between effects". For example, the mean number of opioid pills supplied per capita for each county is included as a variable. 

County-mean demographic variables are not included for a few reasons. The number of observations in the data is small, so I wanted to be careful with adding too many variables into the model. After including these variables in the model, the Akaike information criterion (AIC), a common measure of relative model fit, went up slightly (indicating a worse model fit) and the residual diagnostics (discussed in the Appendix) indicated a worse fit as well. Since these variables were primarily included as controls, I felt comfortable leaving them out for now. 

I use a Poisson distribution (a commonly used distribution to model count data) with a log link and the <code>lme4::glmer</code> function from the <a href="https://cran.r-project.org/web/packages/lme4/index.html" target="_blank">lme4 package</a> to estimate the model. In addition to the variables discussed before, indicator variables for each year of data are included to account for any potential differences in the data gathering process from year to year. 

## Model Results

Below are the resulting estimates for the coefficients of the model. I translated the coefficients to be percent changes in the number of prescription opioid related hospitalizations per 10,000 residents of a county. The variables have also been translated to units that I thought would be more interpretable and add context. A statistically significant term can be interpreted as changing the number of prescription opioid related hospitalizations per 10,000 by x%, controlling for the other variables. For "within effects", this change is for a county in a year, while for "between effects", this change is in the average number of hospitalizations for a county. Bold variables are statistically significat at the 0.05 level.[^5] In parentheses, are the percent change estimates 2 standard errors below and above the point estimate.

<div style="margin-bottom:20px;"></div>

``` {r}
mdl_df <-
  read_arcos_data("poisson-model-data.rds")

read_arcos_data("poisson-pctchange-table.rds")
```

<div style="margin-bottom:20px;"></div>

The main takeaway from the estimates is that there is indeed an association between the supply of opioid pills to a county and its prescription opioid hospitalization rate. More specifically, <strong>controlling for the demographic and economic descriptors, and controlling for the percent of the population that is receiving blind and/or disability SSI benefits, a county in New Jersey with a 5 pills per capita higher average number of prescription opioids supplied can be expected to have `r mdl_df %>% filter(term == "mean_lag_opioids") %>% pull(pctg) %>% round(2)`% more prescription opioid related hospitalizations per 10,000 residents</strong>. 

```{r}
display_arcos_plot("model-plot.rds", static = TRUE)
```

The model also found that an increase in the average median household income for a county is associated with lower average prescription opioid related hospitalizations for that county, controlling for other variables. An increase of 0.1% of a county's residents  receiving SSI Blind and/or Disability benefits is associated with more opioid pill related hospitalizations in that year, controlling for other variables. Additionally, the year 2015 is statistically significant and associated with a higher prescription opioid hospitalization rate. This most likely reflects the change in coding of hospitalization visits in the fourth quarter of 2015, an update of the International Classification of Diseases, Clinical Modification (ICD-CM) to the 10th revision. Prescription rate in the previous year is on the border of statistical significance and it is possible with more data (and therefore more statistical power), its estimate would be significant (of course, the opposite could also be true).

These results are in line with my intuition for the expected relationships between these variables and hospitlizations. However, an unintuitive result shows up with unemployment rate. A higher average unemployment rate for a county is associated with a lower number of prescription opioid related hospitalizations. Although, this relationship might be an indicator of a modeling or data deficiency, it does makes some sense when looking more closely at the interpretation. When interpretting the percent change effect of a variable, the other variables are held constant. After looking at the unemployment rate and hospitalization relationship grouped by binned number of opioids supplied per capita (with the number of pills now seen as being "held constant" within a bin), negative relationships can arise. The 40 group especially has a strong negative relationship. 

<div style="margin-bottom:20px;"></div>

```{r}
display_arcos_plot("unemp-hosp-example.rds", static = TRUE)
```

So although it is unintuitive initially, the negative effect is actually not completely out of left-field, and since the goal of this analysis is to parse out the effect of the opioid pill supply for a county and not specific aspects of its economy, I will leave this the details of this relationship to future work. 

For further examination of the legitamacy of this model and its results, check out the Model Diagnostics in the Appendix. 

## Final Thoughts

Did I create a model that controls for all the institutional effects happening within a county that could impact its number of prescription opioid hospitalizations? Definitely not! Are there issues with the data I use in this analysis. Sure! ACS 1 year estimates can be imprecise, one can imagine all sorts of difficulties with accurately classifying a hospitalization as related to prescription opioid abuse, and for a complex model there are only 168 observations. <strong>However, given the available data, I think there is still enough evidence to say that the number of opioids being supplied to counties in New Jersey is strongly related to an increased number of prescription opioid related hospitalizations.</strong> Even outside of the modeling work done, the raw numbers of opioid pills being supplied to these counties over the years in the data is concerning. Further analysis and more data is needed to exactly quantify the effect of the opioid pill supply on New Jersey counties, and programs like <a href="https://www.cdc.gov/drugoverdose/foa/state-opioid-mm.html?CDC_AA_refVal=https%3A%2F%2Fwww.cdc.gov%2Fdrugoverdose%2Fdata%2Fnonfatal%2Fcdc-esoos.html" target="_blank">CDC's Enhanced State Opioid Overdose Surveillance (ESOOS)</a>, of which New Jersey is a part of, could be a potentially rich source of information for these future analyses and models. 

Ideally, this analysis could be extended to all counties in the United States. Perhaps a future project!

## Appendix

### Choosing a model

For this section, I will assume some familiarity with linear regression and its  associated  terminology.

Every observation in our data is a year in a county, repeated measures that create a nested hiearchy. Observational hierarchies in data violate a standard assumption of linear regression - after controlling for all included variables in a study, the residuals from each observation are independent. The yearly number of hospital visits within each county will be correlated, and therefore the residuals from a linear regression are also likely to be correlated. We are forced to use some other method besides linear regression or its extensions like the general linear model. 

From my research, I found three general approaches to hierarchecal data analysis:

* Generalized Estimating Equations (GEE)
* Fixed-Effects Modeling
* Random-Effects Modeling

GEEs have minimal assumptions on the relationship between independent and dependent variables (always nice), but inferences from this model are restricted to your data's population mean. Although I run a GEE for a robustness check of my results, I ideally wanted to be able to make inferences at the county level (i.e. I'd like to be able to say "I expect a county in New Jersey to have an increase in its prescription-opioid hospitalizations when a variable changes..." over "I expect New Jersey to have an increase in its prescriptions opioid hospitalizations when a county-level variable changes..." ). 

The other two methods, fixed-effects and random-effects models, are closely linked.[^4] For this simple explanation, suffice to say that a fixed-effects model controls for a hierarchical structure by including variables related to each group in the hierarchy (in our case, a binary variable for each county, equalling 1 if the observation is in the county). 

<div style="margin-bottom: 20px"></div>

| County | Year | Control for Atlantic | Control for Bergen | ... |
| :----: | :--: | :------------------: | :----------------: | :-: |
| Atlantic | 2006 | 1 | 0 | ... |
| Atlantic | 2007 | 1 | 0 | ... |
| Bergen   | 2006 | 0 | 1 | ... |

Table: Example of Data Prepped for a Fixed Effects Model

This is nice because any time-invariant effects we are not controlling for in the model are pooled together and controlled for by this county indicator variable. However, you can only estimate within-county effects (how an increase in the yearly prescription rate for a single county effects hospitalizations), not between county effects (how an increase in the overall mean prescription rate of a county effects hospitalizations) as those between county effects are perfectly correlated with the county indicator variable. Random-effects models (aka the mixed effects model, aka the hierarchical linear model, aka the multilevel model, etc.) try to explicitly model the hierarchical differences by allowing estimates for the effect of a variable to vary by group (they are "random" in the sense that each group's version of the effect varies around an estimated mean effect). The classical example of a random effect in a model is the random intercept, where every group is allowed to have its own intercept ($\beta_0$ in linear regression). In the context of this data, each county starts out with different numbers of hospitalizations when their independent variables are set to 0. However, the effect of each variable on hospitalizations is set to be constant across counties.

<div style="margin-bottom:20px;"></div>

``` {r}
set.seed(4321)

x <- rnorm(100)
example_df <-
  map_df(round(rnorm(5), 2), function(intercept) {
    data.frame(Intercept = intercept,
               y = x + intercept,
               x = x)
  }) %>% 
  mutate(Group = paste0("Group ", 
                        as.numeric(as.factor(Intercept)),
                        " (", Intercept, ")")) %>% 
  filter(x > 0)

ggplot(example_df, aes(x, y, group = Group, color = Group)) +
  geom_line() + 
  labs(x = "Independent Variable", y = "Dependent Variable") +
  theme_bw() +
  ggtitle("Example Plot of Random Intercepts with Identical Slopes")
```

For a significantly more in-depth but still accessible account of these two models, I recommend <a href="https://www.cambridge.org/core/services/aop-cambridge-core/content/view/0334A27557D15848549120FE8ECD8D63/S2049847014000077a.pdf/explaining_fixed_effects_random_effects_modeling_of_timeseries_crosssectional_and_panel_data.pdf" target="_blank">Bell and Jones, 2015</a>, which helped me tremendously in understanding the underlying concepts of these models in a practical, straightforward way. Due to my desire to estimate the between effects of variables and for reasons outlined in the aformentioned paper, I chose to model prescription opioid hospitalizations with a random-intercept model.


### Model Diagnostics
To ensure the legitamacy of the results from this model, we need to check some of its assumptions. In ordinary linear regression, this is often done by looking at the residuals. However, with a count outcome, often the data is grouped around a few numbers. With the hierarchical structure of our data, this strong grouping only intensifies. This makes it difficult to check classic pearson or deviance residuals and make informative conclusions. However, the R package <a href="https://cran.r-project.org/web/packages/DHARMa/vignettes/DHARMa.html" target="_blank">DHARMa</a> provides a method to check randomized quantile residuals. Randomized quantile residuals are a simulation based approach to checking model validity. Creating these residuals is done by first estimating an empirical cumulative density function (CDF) from the predicted values of the model. Then the actual observational outcome data is checked against the CDF, and the quantile it falls in is recorded. For example, a recorded quantile of 0.75 means that 75% of the predicted data falls below that observation. The "randomized" part of this process is the transformation of an integer outcome into a smooth CDF, which requires adding a random value to the predictions. This is usually chosen from a random uniform distribution. Intuitively, one would think that if the model is guessing wrong in a non-systematic way then the spread of the observations along the quantiles will be pretty even. Indeed, <a href="https://math.usask.ca/~longhai/researchteam/theses/alithesis.pdf" target="_blank">Sadeghpour (2016)</a> showed that if the model accurately reflects the data generating process of the observations, then the randomized quantile residuals will follow a normal distribution. 

Modeling hospitalization counts with a poisson distribution adds an assumption of equidispersion, that the mean and variance of the residuals from our model are the same. More often than not, models suffer from overdispersion. 

<div style="margin-bottom:20px;"></div>

```{r}
resids <- read_arcos_data("simulated-residuals.rds")
plot(resids)
```

The overdispersion test from the <a href="https://easystats.github.io/performance/reference/check_overdispersion.html" target="_blank">performance</a> package was used to check if the residuals of the model exhibit overdispersion, and the test failed to reject the null hypothesis of equidispersion.

The quantile residuals seem to be normally distributed according to the QQ plot and there is no obvious pattern in the plot of the predicted values and the quantile residuals. The variance of the residuals also stays constant along the range of the model predictions. The black line with the shaded confidence intervals on the Residuals vs. Predicted plot are the results of a quantile regression in DHARMa, showing a comparison of the empirically derived quantiles residuals and the ideal quantile intervals (the default of 0.25, 0.50, and 0.75). The quantile regression is run by regressing the model predictions onto one of the quantiles of the empirically derived residuals. Ideally, one would want to see no relationship between the model predictions and any residual quantile. This is also true of the independent variables. A relationship between an independent variable and a quantile of the residuals could indicate there is an important variable missing from the model or some other misspecification that could mean  bias in the estimates. 

``` {r}
library(DHARMa)

plotResiduals(resids, 
              form = read_arcos_data("poisson-model-object.rds")@frame$lag_opioids, 
              xlab = "Scaled by 100 and Mean Centered: Opioid Pills Supplied per capita in Previous Year")
```

The main variable of interest, opioid pills supplied, does not show any statistically significant quantile deviations (the same is true of the other independent variables). However, there is a quadratic looking pattern. This may indicate that the relationship between opioid supply and hospitalizations could be modeled with a quadratic transformation, or that there is some missing variable(s) accounting for the relationship. I could not get a successful model fit after adding a quadratic opioid supply term, and given the low count of observations, I leave that to future work. If this analysis can be extended to the entire United States, the increased number of observations would allow for more complex relationships to be included in the model. 

As a robustness check of the results of this model, I ran Generalized Estimating Equations (GEE) regression, which has  fewer assumptions but more restrictive interpretations for the hypothesis of interest. The same exact data and variables were included and results were not drastically different. The average opioids per capita of a county had a p-value just outside of the statistically significant range at 0.05, but this is not enough to raise alarm bells, given how sensitive I found the GEE to specification. Interestingly, prescription rate in the previous year is statistically significant in the GEE (it was just on the border of the random intercept model). The other variables that were statistically significant remain the same, with identical directions for their effects. I am much more worried about possible misspecification of the model in terms of ommitted variables, than the distributional assumptions that a GEE gets around, but its results do not take away from the evidence put forth by the random intercept model.

[^1]: The underlying data from the dashboard actually comes from the <a href="https://www.nj.gov/health/healthcarequality/health-care-professionals/njddcs/" target="_blank">NJ Hospital Discharge Data Collection System (NJDDCS)</a>. The NJDDCS collects its data from electronically submitted claims for health care provided in a hospital or electronic submitted exchanges of claims information between payers. Payers in this case being either the individual, business, or insurance company paying for a healthcare service.
[^2]: The actual data supporting these maps and tables comes from the IQVIA Xponent 2006–2018 data, a sample of approximately 49,900 retail (non-hospital) pharmacies, which dispense nearly 92% of all retail prescriptions in the United States. From the CDC Website, a prescription in this data is defined as "an initial or refill prescription dispensed at a retail pharmacy in the sample and paid for by commercial insurance, Medicaid, Medicare, cash or its equivalent. This database does not include mail order prescriptions." Unfortunately, there is no guarantee that the person obtaining a prescription lives in that county, which adds uncertainty to the rate numbers provided by this data. 
[^3]: A more thorough discussion of choosing ACS estimates can be found in Appendix 1 of <a href="https://www.psc.isr.umich.edu/dis/acs/handouts/Compass_Appendix.pdf" target="_blank">University of Michigan's ACS guidelines</a>.
[^4]: The terms fixed effects and random effects have different <a href="https://stats.stackexchange.com/a/4702" target="_blank">meanings and interpretations</a> depending on who is using them. 
[^5]: The prescription rate in the previous year is not quite statistically significant at the 0.05 level, but is close enough that I wanted it to also stand out.