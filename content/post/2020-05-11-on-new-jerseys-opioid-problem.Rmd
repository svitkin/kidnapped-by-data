---
title: On New Jersey's Opioid Problem
author: ''
date: '2020-05-11'
slug: on-new-jerseys-opioid-problem
categories: []
tags:
  - R
  - multilevel modeling
  - plotly
  - ARCOS
  - opioids
---


## The Introduction

I was raised in New Jersey and (at this point, like most people) have friends and family impacted by opioid addiction. I've read news reports on the prescription opioid epidemic, but I've never really done a deep dive to learn more, so I thought a good way to dig in would be with some data and a project. After some research, I found the <a href="https://wpinvestigative.github.io/arcos/" target="_blank">Automation of Reports and Consolidated Orders System (ARCOS) database</a>. The data from ARCOS is owned by the Drug Enforcement Agency (DEA), but was released by the Washington Post on July 16th, 2019, after a long legal battle in conjunction with the Charleston Gazette-Mail of West Virginia. ARCOS tracks the supply side of opioid pills in the United States, with data on every pill sold from a manufacturer to a pharmacy from 2006 to 2014.

For this post, I am going to focus on my home state. However, the opioid epidemic is far reaching, and the ARCOS database is an invaluable resource for understanding it. For national coverage and analysis of ARCOS, check out the <a href="https://www.washingtonpost.com/national/2019/07/20/opioid-files/?arc404=true" target="_blank">Washington Post's collection of articles</a>. For local reports on ARCOS, check out the Washington Post's <a href="https://www.washingtonpost.com/national/2019/08/12/post-released-deas-data-pain-pills-heres-what-local-journalists-are-using-it/" target="_blank">other collection of articles</a>. For an existing analysis of this data for New Jersey, I highly recommend <a href="https://www.nj.com/data/2019/07/tracking-the-15-billion-painkillers-that-flooded-nj-and-sparked-an-epidemic-thats-killed-nearly-20k.html" target="_blank">NJ.com's article</a>, which is filled with some excellent data visualizations and reporting that offers a great summary of how badly impacted New Jersey has been by the flood of prescription pills into its communities. 

All code and data for the analyses from this post can be found at the associated <a href="https://github.com/svitkin/arcos-project" target="_blank">github repository</a>.

## The Hypothesis
I began looking at ARCOS with a broad goal of finding more about the impact of the supply of opioid pills on New Jersey. Over time, my focus began to narrow to understanding the impact on a county level. The ARCOS API wrapper in R has a handy <code>arcos::summarized_county_annual()</code> function that pulls in the number of hydrocodone and oxycodone pills supplied to a county per year. Going forward, when I refer to opioid pills in the context of the ARCOS dataset, I only mean oxycodone and hydrocodone pills. 

After deciding on a county-level analysis, I still needed to find a measurable way to define "impact". Ideally, a public health outcome related to opioid pill abuse. NJ.gov has a <a href="https://www.nj.gov/health/populationhealth/opioid/opioid_hospital.shtml" target="_blank">data dashboard on drug-related hospital visits</a> that has enough overlap with the years of the ARCOS data to be useful to this analysis. With that final piece, a testable hypothesis could be formed: The increasing number of opioid pills supplied to New Jersey counties is associated with an increasing number of prescription opioid related hospital visits. Not the most earth-shattering of hypotheses, but an important one to test and get verifiable quantities on.

## The Data

```{r include=FALSE}
library(here)
library(plotly)
library(dplyr)
library(stringr)
library(widgetframe)
library(DT)

knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)

display_arcos_plot <- function(rds_file, static = FALSE) {
  plt <- 
    readRDS(here("static", "post", "arcos-project-post", "plot-objects", rds_file))
  
  if (!static) {
    frameWidget(plt)  
  } else {
    plt
  }
}

read_arcos_data <- function(rds_file) {
  readRDS(here("static", "post", "arcos-project-post", "data", rds_file))  
}

```

The ARCOS data can be accessed through an <a href="https://github.com/wpinvestigative/arcos-api" target="_blank">API</a>, built in R. As previously mentioned, I used <code>arcos::summarized_county_annual()</code>, which returns the total number of hydrocodone and oxycodone pills shipped to pharmacies and practitioners in a county for each year of data available (2006-2014). Let's look at a summary of the ARCOS data in New Jersey.

<div style="margin-bottom:30px;"></div>

```{r}
nj_data <-
  read.csv(here("static", "post", "arcos-project-post", "data", "nj-regression-data.csv"),
           stringsAsFactors = FALSE)
total_nj_summary <-
  nj_data %>% 
  filter(year %in% 2006:2014) %>% 
  group_by(year) %>% 
  summarise(sum_pills = sum(dosage_unit),
            population = sum(population)) %>% 
  mutate(buyer_county = "All of New Jersey") %>% 
  group_by(buyer_county) %>% 
  summarise(total_pills = sum(sum_pills),
            mean_pills = round(mean(sum_pills/population), 2)) %>% 
  ungroup()
  
county_nj_summary <-
  nj_data %>% 
  filter(year %in% 2006:2014) %>% 
  group_by(buyer_county) %>% 
  summarise(total_pills = sum(dosage_unit),
            mean_pills = round(mean(dosage_unit/population), 2)) %>% 
  ungroup()

bind_rows(total_nj_summary,
          county_nj_summary) %>% 
  mutate(buyer_county = as.factor(buyer_county)) %>% 
  mutate(total_pills = formatC(total_pills, format = "fg", big.mark = ",")) %>% 
  datatable(filter = "top",
            colnames = 
              c("County", "Total # of Pills", "Yearly Average # of Pills per Capita"),
            caption = htmltools::tags$caption(
              style = 'caption-side: bottom; text-align: center;',
              htmltools::em("Hydrocodone and Oxycodone Pills Supplied to New Jersey")),
            rownames = FALSE,
            extensions = 'Buttons', 
            options = list(
              dom = 'Bfrtip',
              buttons = c('copy', 'csv', 'pdf'),
              paging = FALSE,
              scrollY = "200px"
            )) %>% 
  frameWidget()

```


From 2006 to 2014, New Jersey was supplied with `r formatC(total_nj_summary$total_pills, format = "fg", big.mark = ",")` hydrocodone and oxycodone pills. On average, `r total_nj_summary$mean_pills` hydrocodone and oxycodone pills were supplied per capita through those 8 years (per capita rates are obtained with county population estimates from the American Community Survey, discussed in greater detail below). There is quite a bit of range in the profile of counties in terms of prescriptions opioid pills supplied, from `r county_nj_summary %>% filter(mean_pills == min(mean_pills)) %>% pull(buyer_county)` County with `r min(county_nj_summary$mean_pills)` pills per capita supplied on average to `r county_nj_summary %>% filter(mean_pills == max(mean_pills)) %>% pull(buyer_county)` County with `r max(county_nj_summary$mean_pills)` pills per capita supplied on average. However, common across all counties, is an upward trend in the number of pills per capita supplied over time. 

<div style="margin-bottom:20px;"></div>

```{r}
display_arcos_plot("opioid_pills_supplied_per_capita-county.rds")
```


Indeed, every single county in New Jersey had a higher yearly pill supply in 2014 than in 2006. 

The data for prescription opioid related hospital visits comes from a NJ.gov <a href="https://www.nj.gov/health/populationhealth/opioid/opioid_hospital.shtml" target="_blank">dashboard</a>, but its underlying data actually comes from the <a href="https://www.nj.gov/health/healthcarequality/health-care-professionals/njddcs/" target="_blank">NJ Hospital Discharge Data Collection System (NJDDCS)</a>. The NJDDCS collects its data from electronically submitted claims for health care provided in a hospital or electronic submitted exchanges of claims information between payers. Payers in this case being either the individual, business, or insurance company paying for a healthcare service. I scraped the data for the earliest year available, 2007, to one year after the last year in the ARCOS data, 2015. Every county in New Jersey (besides Hunterdon which had no change) had more prescription opioid related hospital visits in 2015 than in 2007. 

<div style="margin-bottom:20px;"></div>

```{r}
display_arcos_plot("x_of_hospital_visits_per_10_000_residents-county.rds")
```

A visual inspection of the relationship between pills supplied and hospital visits shows a generally positive association between the two quantities.

<div style="margin-bottom:20px;"></div>

```{r}
display_arcos_plot("opioid_pills_supplied_per_capita_in_previous_year-w-hosp_visits.rds")
```

There is also the other side of the legal opioid supply, prescriptions. Data for opioid prescription rates by county (the number of prescriptions per 100 people) can be found on the <a href="https://www.cdc.gov/drugoverdose/maps/rxrate-maps.html" target="_blank">Center for Disease Control's (CDC) prescription rate maps</a>. The actual data supporting these maps and tables comes from the IQVIA Xponent 2006â€“2018 data, a sample of approximately 49,900 retail (non-hospital) pharmacies, which dispense nearly 92% of all retail prescriptions in the United States. From the CDC Website, a prescription in this data is defined as "an initial or refill prescription dispensed at a retail pharmacy in the sample and paid for by commercial insurance, Medicaid, Medicare, cash or its equivalent. This database does not include mail order prescriptions." Unfortunately, there is no guarantee that the person obtaining a prescription lives in that county, which adds uncertainty to the rate numbers provided by this data. 

<div style="margin-bottom:20px;"></div>

```{r}
display_arcos_plot("prescription_rate-county.rds")
```

Interestingly, although the supply to pharmacies of hydrocodone and oxycodone pills is increasing, the prescription rates do not always show the same obvious positive trends.

<div style="margin-bottom:20px;"></div>

```{r}
display_arcos_plot("prescription_rate_in_previous_year-w-hosp_visits.rds")
```

Prescription rates, however, do seem to have a positive relationship with the number of prescription opioid-related hospital visits. The one group of visual outliers from this positive trend is Cape May. Out of all the counties, Cape May has the highest percentage of its population aged 60 years or older over the years of the data. It has an unusually high proportion of an age demographic that is more likely to get a prescritpion, but is less likely to be hospitalized for prescription opioid abuse. Although this is just a hunch, and older people can certainly abuse prescription opioids in a way that leads to hospitalization. 

To try and isolate the impact of the number of opioid pills supplied on the number of opioid related hospital visits, I also need to control for the other mechanisms that might impact the number of hospital visits for a county. To best approximate these other factors, I pulled down demographic and economic characteristics of each county. 

For all demographic (and some economic) characteristics, I used the U.S. Census's American Community Survey (ACS) 1 year estimates. The ACS is continually sampled throughout its defined duration, with the 1 year ACS estimates having the smallest sample size of the available ACS datasets (the others being the 3 and 5 year). However, since we are interested in individual years and changes between those years, the 1 year estimates are more suited to our data (except for race categories in Cape May county in 2011, which had missing data for its 1 year estimate and was filled in with the 5 year estimate instead). A more thorough discussion of choosing ACS estimates can be found in Appendix 1 of <a href="https://www.psc.isr.umich.edu/dis/acs/handouts/Compass_Appendix.pdf" target="_blank">University of Michigan's ACS guidelines</a>. 

Using the <a href="https://walkerke.github.io/tidycensus/" target="_blank">tidycensus</a> package, I obtained distribution data on race, age, sex, and median household income for each county from 2006 to 2015. Under the <a href="https://www.census.gov/content/dam/Census/library/publications/2018/acs/acs_general_handbook_2018_ch10.pdf" target="_blank">recommendation of the U.S. Census</a>, I used the annual averages from the <a href="https://www.bls.gov/cpi/research-series/home.htm" target="_blank">Consumer Price Index research series</a> to translate the median incomes to 2015 U.S. dollars.

Unemployment rates for each county, as another proxy for economic conditions, were pulled from the <a href="https://www.bls.gov/lau/" target="_blank">Bureau of Labor Statistics (BLS) Local Area Unemployment tables</a>.

Ideally, one would also want to know how many people have a legitamate medical need for opioid pills. This is a <a href="https://www.cdc.gov/drugoverdose/prescribing/guideline.html" target="_blank">complex question</a> with an inherent amount of uncertainty around how much pain is appropriate for someone to be prescribed opiates. Although it is not ideal, I use the Social Security Administration's data on the number of recipients of social security benefits for blindness and/or disability. This is not an ideal measurement, as this pools together numbers for many different disabilities and misses members of the population who are not receiving benefits but might have a medical need for an opioid prescription. However, it is currently the best approximation I could find for this concept.

<div style="margin-bottom:20px;"></div>

```{r}
display_arcos_plot("percent_of_residents_receiving_ssi_blind_and_or_disability_benefits-w-hosp_visits.rds")
```

## The Model
For this section, I will assume some familiarity with linear regression and its  associated  terminology. 

### An Introduction
The data we've been looking at so far, county level observations repeatedly measured for each year, naturally creates a hierarchy (years nested in counties). Observational hierarchies in data violate a standard assumption of linear regression - after controlling for all included variables in a study, the residuals from each observation are independent. The yearly number of hospital visits within each county will be correlated, and therefore the residuals from a linear regression are also likely to be correlated. We are forced to use some other method besides linear regression or its extensions like the general linear model. 

I am a newcomer to hierarchical data analysis, but from my research I found there to be three general approaches. One of the methods, generalized estimating equations (GEE), has minimal assumptions on the relationship between independent and dependent variables (always nice), but inferences from this model are restricted to your data's population mean. Although I run a GEE for a robustness check of my results, I ideally wanted to be able to make inferences at the county level (i.e. I'd like to be able to say "I expect a county in New Jersey to have an increase in its prescription-opioid hospitalizations when a variable changes..." over "I expect New Jersey to have an increase in its prescriptions opioid hospitalizations when a county-level variable changes..." ). 

The other two methods, fixed effects and random effects models, are closely linked. The terms fixed effects and random effects have different <a href="https://stats.stackexchange.com/a/4702" target="_blank">meanings and interpretations</a> depending on who is using them. For our simple explanation, suffice to say that a fixed-effects model controls for a hierarchical structure by including variables related to each group in the hierarchy (in our case, a binary variable for each county, equalling 1 if the observation is in the county). 

<div style="margin-bottom: 20px"></div>

| County | Year | Control for Atlantic | Control for Bergen | ... |
| :----: | :--: | :------------------: | :----------------: | :-: |
| Atlantic | 2006 | 1 | 0 | ... |
| Atlantic | 2007 | 1 | 0 | ... |
| Bergen   | 2006 | 0 | 1 | ... |

Table: Example of Data Prepped for a Fixed Effects Model

This is nice because any time-invariant effects we are not controlling for in the model are pooled together and controlled for by this county indicator variable. However, you can only estimate within-county effects (an increase in the yearly prescription rate for a single county), not between county effects (an increase in the overall mean prescription rate of a county) as those between county effects are perfectly correlated with the county indicator variable. Random effects models (aka the mixed effects model, aka the hierarchical linear model, aka the multilevel model, etc.) try to explicitly model the hierarchical differences by allowing estimates for the effect of a variable to vary by group (they are "random" in the sense that each group's version of the effect varies around an estimated mean effect). The classical example of a random effect in a model is the random intercept, where every group is allowed to have its own intercept ($\beta_0$ in linear regression). In the context of our data, each county starts out with different numbers of hospitalizations when their independent variables are set to 0. However, the effect of each variable on hospitalizations is set to be constant across counties.

<div style="margin-bottom:20px;"></div>

``` {r}
library(purrr)
library(dplyr)
library(tidyr)
library(ggplot2)
set.seed(4321)

x <- rnorm(100)
example_df <-
  map_df(round(rnorm(5), 2), function(intercept) {
    data.frame(Intercept = intercept,
               y = x + intercept,
               x = x)
  }) %>% 
  mutate(Group = paste0("Group ", 
                        as.numeric(as.factor(Intercept)),
                        " (", Intercept, ")")) %>% 
  filter(x > 0)

ggplot(example_df, aes(x, y, group = Group, color = Group)) +
  geom_line() + 
  labs(x = "Independent Variable", y = "Dependent Variable") +
  theme_bw() +
  ggtitle("Example Plot of Random Intercepts with Identical Slopes")
```

For a significantly more in-depth but still accessible account of these two models, I recommend <a href="https://www.cambridge.org/core/services/aop-cambridge-core/content/view/0334A27557D15848549120FE8ECD8D63/S2049847014000077a.pdf/explaining_fixed_effects_random_effects_modeling_of_timeseries_crosssectional_and_panel_data.pdf" target="_blank">Bell and Jones, 2015</a>, which helped me tremendously in understanding the underlying concepts of these models in a practical, straightforward way. For reasons discussed in the aformentioned paper, as well as my own desire to estimate the between effects of variables, I chose to model prescription opioid hospitalizations with a random-effects model.

### A Model

The  years of hospitalization data span from 2008 to 2015, while the ARCOS data goes from 2006 to 2014. This presented an issue when creating the model. If we simply took the number of county-year observations from the overlap of these two datasets, we would have only 147 observations for the 21 counties in New Jersey, a lot less than we started with. And we didn't start with a lot of data. So partially motivated by trying to maximize the number of observations, and partially because there could legitamately be a lag in the effect of opiate pill supply (dependence on a substance that could lead to an eventual hospitalization takes some nonzero amount of time to develop), I lagged the ARCOS data by one year to match up with the hospitalization data. Prescription rate data is also lagged by one year to match up with the ARCOS data. Despite the change, we do still lose the 2006 year of ARCOS data. So with that, we are left with 168 observations in our final dataset, 21 counties and 8 years of data, from 2008 to 2015.

I use a Poisson distribution (a commonly used distribution to model count data) with a log link and a random intercept to model number of hospitaliztions, and the <code>lme4::glmer</code> from the <a href="https://cran.r-project.org/web/packages/lme4/index.html" target="_blank">lme4 package</a> to estimate the model. Indicator variables for each year of data are included to account for any potential differences in the data gathering process from year to year. I mentioned earlier that I want to estimate "between effects", variables that measure differences between counties, not just changes from year-to-year within a county. To do this, I use the "within-between" formulation found in <a href="https://www.cambridge.org/core/services/aop-cambridge-core/content/view/0334A27557D15848549120FE8ECD8D63/S2049847014000077a.pdf/explaining_fixed_effects_random_effects_modeling_of_timeseries_crosssectional_and_panel_data.pdf" target="_blank">Bell and Jones, 2015</a>, itself a modification of the formula found in <a href="https://www.jstor.org/stable/1913646?seq=1" target="_blank">Mundlak, 1978</a>, and county-mean center all the variables and add a county-mean version of almost every variable to the model. For example, the mean unemployment rate over the years of the data for a county is subtracted from each year of unemployment rate data for that county, and the county-level mean of unemployment rate is also included as a variable. The county-mean of the demographic variables are not included for a few reasons. The number of observations in the data is small so I wanted to be careful with adding too many variables into the model. The Akaike information criterion (AIC), a common measure of relative model fit, went up slightly when adding these variables (indicating a worse model fit) and the residual diagnostics (discussed below) indicated a worse fit as well. Since these variables were primarily included as controls, I felt comfortable leaving them out for now. 

## Results

Below are the resulting estimates for the coefficients of the model. I translated the coefficients to be percent changes in the number of prescription opioid related hospitalizations per 10,000 residents of a county. The variables have also been translated to units that I thought would be more interpretable and add context. A statistically significant term can be interpreted as changing the number of prescription opioid related hospitalizations per 10,000 by x%, controlling for the other variables. For "within effects", this change is for a county in a year, while for "between effects", this change is in the average number of hospitalizations for a county. Bold variables are statistically significat at the 0.05 level. In parentheses, are the percent change estimates 2 standard errors below and above the point estimate.

<div style="margin-bottom:20px;"></div>

``` {r}
mdl_df <-
  read_arcos_data("poisson-model-data.rds")

read_arcos_data("poisson-pctchange-table.rds")
```

<div style="margin-bottom:20px;"></div>

The main takeaway from the estimates is that there is indeed an association between the supply of opioid pills to a county and its prescription opioid hospitalization rate. More specifically, controlling for the demographic and economic descriptors, and controlling for the percent of the population that is receiving blind and/or disability SSI benefits, a county in New Jersey with a 5 pills per capita higher average number of prescription opioids supplied can be expected to have `r mdl_df %>% filter(term == "mean_lag_opioids") %>% pull(pctg) %>% round(2)`% more prescription opioid related hospitalizations per 10,000 residents. These results are a piece of evidence in favor of the original hypothesis: that prescription opioid pills supplied to a New Jersey county are associated with an increase in prescription opioid related hospitalizations. 

```{r}
display_arcos_plot("model-plot.rds", static = TRUE)
```

Looking at some of the other significant variables, an increase in the average median household income for a county is associated with lower average prescription opioid related hospitalizations, controlling for other variables. An increase in 0.1 of a percent of residents receiving SSI Blind and/or Disability benefits in a year is associated with more hospitalizations in that year, controlling for other variables. Additionally, the year 2015 is statistically significant and associated with a higher prescription opioid hospitalization rate. This most likely reflects the change in coding of hospitalization visits in the fourth quarter of 2015, an update of the International Classification of Diseases, Clinical Modification (ICD-CM) to the 10th revision. Prescription rate in the previous year is on the border of statistical significance and it is possible with more data (and therefore more statistical power), its estimate would be significant.

These results are in line with my intuitioin for the expected relationships between these variables. However, an unintuitive result shows up with unemployment rate. A higher average unemployment rate for a county is associated with a lower number of prescription opioid related hospitalizations. Although, this relationship might be an indicator of model validity, it makes more sense when looking more closely at the interpretation. Controlling for other variables means that the other variables are held constant. For example, when looking at the unemployment rate and hospitalization relationship grouped by binned number of opioids supplied per capita, negative relationships can arise within a bin ( the 40 group especially has a strong negative relationship). 

<div style="margin-bottom:20px;"></div>

```{r}
display_arcos_plot("unemp-hosp-example.rds", static = TRUE)
```

So although it is unintuitive initially, the negative effect is actually not completely out of left-field. 

## Diagnostics
To ensure the legitamacy of the results from this model, we need to check some of its assumptions. In ordinary linear regression, this is often done by looking at the residuals. However, with a count outcome, often the data is grouped around a few numbers. With the hierarchical structure of our data, this strong grouping only intensifies. This makes it difficult to check classic pearson or deviance residuals and make informative conclusions. However, the R package <a href="https://cran.r-project.org/web/packages/DHARMa/vignettes/DHARMa.html" target="_blank">DHARMa</a> provides a method to check randomized quantile residuals. Randomized quantile residuals are a simulation based approach to checking model validity. Creating these residuals is done by first estimating an empirical cumulative density function (CDF) from the predicted values of the model. Then the actual observational outcome data is checked against the CDF, and the quantile it falls in is recorded. For example, a recorded quantile of 0.75 means that 75% of the predicted data falls below that observation. The "randomized" part of this process is the transformation of an integer outcome into a smooth CDF, which requires adding a random value to the predictions. This is usually chosen from a random uniform distribution. Intuitively, one would think that if the model is guessing wrong in a non-systematic way then the spread of the observations along the quantiles will be pretty even. Indeed, <a href="https://amstat.tandfonline.com/doi/abs/10.1080/10618600.1996.10474708" target="_blank">Dunn, K. P., and Smyth, G. K. (1996)</a> showed that if the model accurately reflects the data generating process of the observations, then the randomized quantile residuals will follow a normal distribution. 

Central to our assumption of modelling the hospitalization counts with a poisson distribution is one of equidispersion, that the mean and variance of the predicted values from our model are the same. More often than not, models suffer from overdispersion. This can also be checked with the DHARMa package when estimating the randomized quantile residuals. 

<div style="margin-bottom:20px;"></div>

```{r}
resids <- read_arcos_data("simulated-residuals.rds")
plot(resids)
```

The overdispersion test from DHARMa does not find that the simulated and observed dispersions are statistically significantly different. Additionally, the overdispersion test from the <a href="https://easystats.github.io/performance/reference/check_overdispersion.html" target="_blank">performance</a> package was used to further check if the residuals of the model exhibit overdispersion, and the test failed to reject the null hypothesis of equidispersion.

The quantile residuals seem to be normally distributed according to the QQ plot and there is no obvious pattern in the plot of the predicted values and the quantile residuals. The variance of the residuals also stays constant along the range of the model predictions. The black line with the shaded confidence intervals on the Residuals vs. Predicted plot are the results of a quantile regression in DHARMa, showing a comparison of the empirically derived quantiles residuals and the ideal quantile intervals (the default of 0.25, 0.50, and 0.75). The quantile regression is run by regressing the model predictions onto one of the quantiles of the empirically derived residuals. Ideally, one would want to see no relationship between the model predictions and any residual quantile. This is also true of the independent variables. A relationship between an independent variable and a quantile of the residuals could indicate there is an important variable missing from the model or some other misspecification that could mean  bias in the estimates. 

The plots of the residuals against the independent variables of the model in this post do not show any statistically significant deviations. They are located in the Additional Diagnostic Plots section at the end of this post. However, visual relationships do exist in these plots, showing potential biases in this model. 

As a robustness check of the results of this model, I ran a GEE, which has  fewer assumptions but more restrictive interpretations for the hypothesis of interested. Results were not drastically different. The average opioids per capita of a county had a p-value just outside of the statistically significant range at 0.05, but this is not enough to raise alarm bells. Interestingly, prescription rate in the previous year is statistically significant in the GEE (it was just on the border of the random intercept model). The other variables that were statistically significant remain the same, with identical directions for their effects. I am much more worried about possible misspecification of the model in terms of ommitted variables, than the distributional assumptions that a GEE gets around, but its results do not take away from the evidence put forth by the random intercept model.

## Conclusion



## Dianostics
plot higher (intercept differences) and lower level residuals against each variable, find correlations between higher and lower level residuals and each variable

## Conclusion

## References

## Appendix
### Data Collection Methodology
### Multilevel Modeling
#### A Brief Introduction
From this point forward, I will assume the reader's familiarity with linear regression and its assumptions.

The hierarchy in the data, 9 yearly observations nested in 21 counties, lends itself to a type of model known by many names, but here we will refer to as the multilevel model. In trying to construct a model around this type of data, it is important to take its hierarchical nature into account. Linear regression assumes that after controlling for all included variables in a study, the residuals from each observation are independent. This is not the case with our data and for most data with a grouping structure. The yearly supply of pills is correlated within each county, and therefore the residuals from a linear regression are likely to be correlated as well. 

Statistical studies, often depending on field, have taken different approaches to modelling hierarchical data. One approach, the fixed-effects model, "controls" for the hierarchy by inculding a dummy (binary) variable for each grouping. However, the purely fixed-effects approach does not allow one to find between-group effects of the variables. In our case, we could not model county-level differences in oxycodone and hydrocodone pill supply, something that is of significant interest. The multilevel model explicitly models these between county differences through the inclusion of "random effects". 

I found the term "random effects" confusing when I first encountered it. It does not mean that the effects you are modelling have anything to do with random chance or circumstance (although their observation could). Rather, they are "random", because they represent a variable's variance for each group. Where in a traditional linear regression, a single coefficient is estimated for each variable, in a multilevel model, that same coefficient exists, but additional coefficents are included for each group. For example, the most basic "random effect" one can add to a multilevel model is a random intercept. 

TODO: Put mathematical notation here.


#### The within-between model
So why not use the multilevel model in every situation involving hierarchical data? Often times, multilevel models suffer from endogeneity, the residuals of the regression are correlated with one or more of the independent variables. 

#### Estimation



### Additional Plots

